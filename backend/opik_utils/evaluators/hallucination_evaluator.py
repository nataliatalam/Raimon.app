"""
Hallucination detector for LLM agent outputs.

Detects when agents generate false, contradictory, or unfounded information.
Uses pattern matching and content validation to identify hallucinations.
"""

from typing import Dict, Any, Optional
from opik_utils.evaluators.base import BaseEvaluator, EvaluationScore
import re


class HallucinationEvaluator(BaseEvaluator):
    """
    Evaluates whether an agent output contains hallucinations.

    Hallucinations are factually incorrect, contradictory, or unfounded information
    generated by the LLM without basis in the provided context.

    Scoring:
    - 1.0: No hallucinations detected
    - 0.5-0.9: Minor hallucinations or unverified claims
    - 0.0-0.5: Significant hallucinations present
    """

    def __init__(self):
        """Initialize the hallucination evaluator."""
        super().__init__(
            name="hallucination_evaluator",
            description="Detects hallucinations in agent outputs"
        )
        # Common hallucination patterns
        self.hallucination_patterns = [
            r"(?:I'm|I am)\s+(?:not\s+)?sure",  # Uncertain claims
            r"(?:probably|likely|maybe)\s+(?:the|a)",  # Qualified statements without evidence
            r"as\s+(?:I\s+)?remember",  # False recollection
            r"I\s+(?:made|create|generate)d",  # False self-reference to creating data
        ]

    def evaluate(
        self,
        output: Any,
        ground_truth: Optional[Any] = None,
        context: Optional[Dict[str, Any]] = None,
    ) -> EvaluationScore:
        """
        Evaluate agent output for hallucinations.

        Args:
            output: Agent output (typically a dict with 'message', 'data', etc.)
            ground_truth: Reference data to check against
            context: Additional context about the task

        Returns:
            EvaluationScore with hallucination assessment
        """
        if not self.validate_output(output):
            return EvaluationScore(
                score=0.0,
                reasoning="Output format invalid or missing",
                metadata={"error": "invalid_format"}
            )

        output = self.preprocess_output(output)

        # Extract text content
        text_content = self._extract_text(output)
        if not text_content:
            return EvaluationScore(
                score=1.0,
                reasoning="No text content to evaluate",
                metadata={"content_type": "empty"}
            )

        # Detect hallucination indicators
        hallucination_score = self._detect_hallucinations(text_content, context)

        # Verify against ground truth if provided
        if ground_truth:
            verification_score = self._verify_against_ground_truth(text_content, ground_truth)
            hallucination_score = (hallucination_score + verification_score) / 2

        # Invert: high hallucination detection â†’ low score
        final_score = 1.0 - hallucination_score

        reasoning = self._generate_reasoning(hallucination_score, text_content)
        metadata = {
            "hallucination_indicator_score": hallucination_score,
            "text_length": len(text_content),
            "has_ground_truth": ground_truth is not None,
        }

        score = EvaluationScore(
            score=final_score,
            reasoning=reasoning,
            metadata=metadata
        )

        return self.postprocess_score(score)

    def _extract_text(self, output: Any) -> str:
        """Extract text content from various output formats."""
        if isinstance(output, str):
            return output
        elif isinstance(output, dict):
            # Try common keys
            for key in ["message", "text", "content", "response", "output"]:
                if key in output and isinstance(output[key], str):
                    return output[key]
            # Fallback: concatenate all string values
            return " ".join(
                str(v) for v in output.values()
                if isinstance(v, str)
            )
        elif hasattr(output, "message") and isinstance(output.message, str):
            return output.message
        else:
            return str(output)

    def _detect_hallucinations(
        self,
        text: str,
        context: Optional[Dict[str, Any]] = None
    ) -> float:
        """
        Detect hallucination indicators in text.

        Returns:
            Score 0.0-1.0 where 1.0 = certain hallucination, 0.0 = no hallucinations
        """
        hallucination_count = 0
        total_patterns = len(self.hallucination_patterns)

        # Check for common hallucination patterns
        for pattern in self.hallucination_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                hallucination_count += 1

        # Check for contradictions
        if self._has_contradictions(text):
            hallucination_count += 1
            total_patterns += 1

        # Check for implausible claims
        if context and self._has_implausible_claims(text, context):
            hallucination_count += 1
            total_patterns += 1

        return min(1.0, hallucination_count / max(total_patterns, 1))

    def _has_contradictions(self, text: str) -> bool:
        """Check for internal contradictions in the text."""
        # Simple check for contradictory phrases
        contradictions = [
            (r"yes", r"no"),
            (r"true", r"false"),
            (r"always", r"never"),
            (r"increase", r"decrease"),
        ]

        text_lower = text.lower()
        for pos, neg in contradictions:
            if re.search(pos, text_lower) and re.search(neg, text_lower):
                return True
        return False

    def _has_implausible_claims(
        self,
        text: str,
        context: Dict[str, Any]
    ) -> bool:
        """
        Check for claims that contradict provided context.

        Args:
            text: Output text
            context: Context dict with expected constraints/data

        Returns:
            True if implausible claims detected
        """
        if not context:
            return False

        # Check if text claims something that contradicts context
        if "task_candidates" in context:
            candidates = context["task_candidates"]
            if candidates and len(candidates) > 0:
                # If there are candidates but text says "no tasks available"
                if re.search(r"no\s+tasks?\s+available", text, re.IGNORECASE):
                    return True

        if "user_energy" in context:
            energy = context.get("user_energy")
            # Check for energy-contradictory claims
            if energy and energy < 3:
                if re.search(r"high\s+energy|very\s+energetic", text, re.IGNORECASE):
                    return True

        return False

    def _verify_against_ground_truth(self, text: str, ground_truth: Any) -> float:
        """
        Verify text against ground truth reference.

        Returns:
            Score 0.0-1.0 where 1.0 = completely wrong, 0.0 = verified correct
        """
        if isinstance(ground_truth, str):
            # Simple string matching
            if ground_truth.lower() in text.lower():
                return 0.0  # Matches ground truth = no hallucination
            else:
                return 0.8  # Doesn't match = likely hallucination
        elif isinstance(ground_truth, dict):
            # Check if output contains expected keys/values
            mismatches = 0
            total_checks = 0
            for key, expected_val in ground_truth.items():
                total_checks += 1
                if str(expected_val).lower() not in text.lower():
                    mismatches += 1
            return mismatches / max(total_checks, 1)
        else:
            return 0.0  # Can't verify

    def _generate_reasoning(self, hallucination_score: float, text: str) -> str:
        """Generate human-readable reasoning for the score."""
        if hallucination_score < 0.2:
            return "Output appears factually grounded with no significant hallucinations detected."
        elif hallucination_score < 0.5:
            return "Output contains minor uncertain claims or unverified statements."
        elif hallucination_score < 0.8:
            return "Output contains several potential hallucinations or contradictions."
        else:
            return "Output shows significant hallucinations or factually incorrect information."
