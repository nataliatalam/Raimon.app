"""
Motivation message quality evaluator.

Evaluates the quality of motivation messages generated by the motivation agent
using a rubric that checks for empathy, actionability, and personalization.
"""

from typing import Dict, Any, Optional
from opik_utils.evaluators.base import BaseEvaluator, EvaluationScore
import re


class MotivationRubricEvaluator(BaseEvaluator):
    """
    Evaluates motivation message quality using a multi-dimensional rubric.

    Dimensions:
    - Empathy: Does the message acknowledge the user's state/feelings?
    - Actionability: Does it provide clear next steps?
    - Personalization: Is it tailored to the user's context?
    - Tone: Is it encouraging and positive?
    - Relevance: Does it relate to the current task/situation?

    Scoring:
    - 1.0: Excellent motivation with all dimensions strong
    - 0.75: Good motivation with most dimensions strong
    - 0.5: Fair motivation with some weak dimensions
    - 0.25: Poor motivation with multiple weak dimensions
    - 0.0: Ineffective motivation message
    """

    def __init__(self):
        """Initialize the motivation rubric evaluator."""
        super().__init__(
            name="motivation_rubric",
            description="Evaluates quality of motivation messages using multi-dimensional rubric"
        )

        # Empathy indicators
        self.empathy_words = [
            r"understand", r"feel", r"know", r"realize", r"sense",
            r"acknowledge", r"recognize", r"appreciate", r"tough", r"challenging"
        ]

        # Actionability indicators
        self.action_words = [
            r"start", r"begin", r"try", r"break.*down", r"step", r"next",
            r"take", r"do", r"attempt", r"focus", r"prioritize", r"let's"
        ]

        # Personalization indicators
        self.personalization_words = [
            r"you", r"your", r"i\s+think", r"specifically", r"especially", r"based"
        ]

        # Positive tone indicators
        self.positive_words = [
            r"great", r"excellent", r"amazing", r"wonderful", r"fantastic",
            r"proud", r"capable", r"strong", r"confident", r"achievable"
        ]

    def evaluate(
        self,
        output: Any,
        ground_truth: Optional[Any] = None,
        context: Optional[Dict[str, Any]] = None,
    ) -> EvaluationScore:
        """
        Evaluate motivation message quality.

        Args:
            output: Motivation message (dict or string)
            ground_truth: Optional reference motivation for comparison
            context: Context including user mood, energy, task info

        Returns:
            EvaluationScore with rubric evaluation
        """
        if not self.validate_output(output):
            return EvaluationScore(
                score=0.0,
                reasoning="Output format invalid or empty",
                metadata={"error": "invalid_format"}
            )

        output = self.preprocess_output(output)
        message = self._extract_message(output)

        if not message or len(message.strip()) < 10:
            return EvaluationScore(
                score=0.0,
                reasoning="Message too short or empty to evaluate",
                metadata={"message_length": len(message) if message else 0}
            )

        # Evaluate each dimension
        empathy_score = self._evaluate_empathy(message, context)
        actionability_score = self._evaluate_actionability(message, context)
        personalization_score = self._evaluate_personalization(message, context)
        tone_score = self._evaluate_tone(message)
        relevance_score = self._evaluate_relevance(message, context)

        # Weighted average (empathy and actionability are most important)
        weights = {
            "empathy": 0.25,
            "actionability": 0.25,
            "personalization": 0.20,
            "tone": 0.20,
            "relevance": 0.10,
        }

        final_score = (
            empathy_score * weights["empathy"]
            + actionability_score * weights["actionability"]
            + personalization_score * weights["personalization"]
            + tone_score * weights["tone"]
            + relevance_score * weights["relevance"]
        )

        reasoning = self._generate_reasoning(
            empathy_score,
            actionability_score,
            personalization_score,
            tone_score,
            relevance_score
        )

        metadata = {
            "empathy_score": round(empathy_score, 2),
            "actionability_score": round(actionability_score, 2),
            "personalization_score": round(personalization_score, 2),
            "tone_score": round(tone_score, 2),
            "relevance_score": round(relevance_score, 2),
            "message_length": len(message),
        }

        score = EvaluationScore(
            score=round(final_score, 2),
            reasoning=reasoning,
            metadata=metadata
        )

        return self.postprocess_score(score)

    def _extract_message(self, output: Any) -> str:
        """Extract message text from various output formats."""
        if isinstance(output, str):
            return output
        elif isinstance(output, dict):
            for key in ["message", "text", "content", "motivation", "response"]:
                if key in output and isinstance(output[key], str):
                    return output[key]
            return " ".join(str(v) for v in output.values() if isinstance(v, str))
        elif hasattr(output, "message") and isinstance(output.message, str):
            return output.message
        else:
            return str(output)

    def _evaluate_empathy(self, message: str, context: Optional[Dict[str, Any]]) -> float:
        """Evaluate empathy dimension."""
        message_lower = message.lower()
        empathy_count = sum(
            1 for word in self.empathy_words
            if re.search(word, message_lower)
        )

        score = min(1.0, empathy_count / 3)  # 3 empathy markers = perfect score

        # Bonus: if context indicates low mood/energy, check for acknowledgment
        if context:
            if context.get("user_mood") in ["frustrated", "tired", "stuck"]:
                if empathy_count >= 2:
                    score = min(1.0, score + 0.2)
                elif empathy_count == 0:
                    score *= 0.5

        return score

    def _evaluate_actionability(self, message: str, context: Optional[Dict[str, Any]]) -> float:
        """Evaluate actionability dimension."""
        message_lower = message.lower()
        action_count = sum(
            1 for word in self.action_words
            if re.search(word, message_lower)
        )

        score = min(1.0, action_count / 3)  # 3 action markers = perfect score

        # Check for specific steps or breaks
        if re.search(r"\d+\s*\.", message):  # Numbered list
            score = min(1.0, score + 0.2)
        elif re.search(r"first.*second|then|next", message_lower):
            score = min(1.0, score + 0.15)

        return score

    def _evaluate_personalization(self, message: str, context: Optional[Dict[str, Any]]) -> float:
        """Evaluate personalization dimension."""
        message_lower = message.lower()
        personalization_count = sum(
            1 for word in self.personalization_words
            if re.search(word, message_lower)
        )

        score = min(1.0, personalization_count / 4)  # 4 personalization markers = perfect

        # Bonus if references task or user context
        if context:
            if "task_title" in context and context["task_title"]:
                if context["task_title"].lower() in message_lower:
                    score = min(1.0, score + 0.15)

            if "user_name" in context and context["user_name"]:
                if context["user_name"].lower() in message_lower:
                    score = min(1.0, score + 0.2)

        return score

    def _evaluate_tone(self, message: str) -> float:
        """Evaluate positive tone dimension."""
        message_lower = message.lower()

        # Positive indicators
        positive_count = sum(
            1 for word in self.positive_words
            if re.search(word, message_lower)
        )

        # Negative indicators (should be absent or very few)
        negative_patterns = [
            r"(?:can't|cannot|won't|won't)\s+do",
            r"impossible|hopeless|give\s+up",
            r"(?:you're|you\s+are)\s+(?:lazy|stupid|incompetent)"
        ]
        negative_count = sum(
            1 for pattern in negative_patterns
            if re.search(pattern, message_lower)
        )

        score = min(1.0, positive_count / 3)  # 3 positive markers = perfect
        score -= (negative_count * 0.3)  # Penalize negative language
        score = max(0.0, score)

        return score

    def _evaluate_relevance(self, message: str, context: Optional[Dict[str, Any]]) -> float:
        """Evaluate relevance to current context."""
        if not context:
            return 0.5  # Neutral if no context

        score = 0.5  # Start neutral

        # Check relevance to event type
        if "event_type" in context:
            event_type = context["event_type"].lower()
            if event_type == "task_completion" and any(
                w in message.lower() for w in ["congrat", "finish", "done", "complet"]
            ):
                score += 0.3
            elif event_type == "stuck" and any(
                w in message.lower() for w in ["stuck", "difficult", "break", "step"]
            ):
                score += 0.3
            elif event_type == "motivation" and any(
                w in message.lower() for w in ["you", "can", "strong", "capable"]
            ):
                score += 0.3

        return min(1.0, score)

    def _generate_reasoning(
        self,
        empathy: float,
        actionability: float,
        personalization: float,
        tone: float,
        relevance: float
    ) -> str:
        """Generate human-readable rubric feedback."""
        feedback = []

        if empathy >= 0.75:
            feedback.append("Shows strong empathy for user's situation")
        elif empathy >= 0.5:
            feedback.append("Adequate empathy; could better acknowledge user's feelings")
        else:
            feedback.append("Limited empathy; should better acknowledge user's emotions")

        if actionability >= 0.75:
            feedback.append("Provides clear, actionable next steps")
        elif actionability >= 0.5:
            feedback.append("Decent actionability; could be more specific")
        else:
            feedback.append("Low actionability; should provide specific next steps")

        if personalization >= 0.75:
            feedback.append("Well-personalized to user context")
        elif personalization >= 0.5:
            feedback.append("Somewhat personalized; could reference more specifics")
        else:
            feedback.append("Generic message; should reference user/task context")

        if tone >= 0.75:
            feedback.append("Positive and encouraging tone")
        else:
            feedback.append("Tone could be more encouraging")

        return ". ".join(feedback)
